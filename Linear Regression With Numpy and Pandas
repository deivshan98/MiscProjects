#easy linear regression with numpy and pandas. one factor model

#library
import matplotlib.pyplot as plt
plt.style.use('ggplot')
%matplotlib inline
import numpy as np
import pandas as pd
import seaborn as sns
plt.rcParams['figure.figsize'] = (12,8)

#load data
data = pd.read_csv('bike_profit.csv')
data.head()

#visualization of data using Seaborn which is a wrapper over matplotlib
ax = sns.scatterplot(x = 'Population', y = 'Profit', data=data)
ax.set_title('Profit in $10000s vs City Population in 10000s'); #here we can use set_title because sns is a wrapper for matplotlib

#compute the cost function
def cost_function(x, y, theta):
    m = len(y)
    y_pred = x.dot(theta)
    error = (y_pred - y) ** 2
    return 1/(2*m)*np.sum(error)

m = data.Population.values.size
x = np.append(np.ones((m,1)), data.Population.values.reshape(m,1), axis = 1)
y = data.Profit.values.reshape(m,1)
theta = np.zeros((2,1))

cost_function(x, y , theta)

#gradient descent
def gradient_descent(x,y,theta,alpha,iterations):
    m = len(y)
    costs=[]
    for i in range(iterations):
        y_pred = x.dot(theta)
        error = np.dot(x.transpose(), y_pred - y))
        theta -= alpha * 1/m * error
        costs.append(cost_function(x,y,theta))
    return theta, costs
theta, costs = gradient_descent(x, y, theta, alpha - 0.01, iterations = 2000)
print("h(x) = {} + {}x1".format(str(round(theta[0,0],2)), str(roun(theta[1,0], 2))))

#visualising the cost function
from mpl_toolkits.mplot3d import Axes3D
theta_0 = np.linspace(-10,10,100)
theta_1 = np.linspace(-1,4,100)
cost_values = np.zero((len(theta_0), len(theta_1)))

for i in range(len(theta_0)):
    for j in range(len(theta_1)):
        t = np.array([theta_0[i], theta_1[j]])
        cost_values[i,j] = cost_function(x,y,t)

fig = plt.figure(figsize = (12,8))
ax = fig.gca(projection = '3d')
surf = ax.plot_surface(theta_0, theta_1, cost_values, cmap= 'viridis')
fig.colorbar(surf, shrink = 0.5, aspec = 5)
plt.xlabel("$\theta_0$")
plt.ylabel("$\theta_1$")
ax.biew_init(30,300)
plt.show()

#plotting the convergence
plt.plot(costs)
plt.xlabel("iterations")
plt.ylabel("$J\Theta)$")
plt.title("Values of the cost function over iterations of gradient descent")

#training data with linear Regression Fit
theta = np.squeeze(theta)
sns.scatterplot(x = 'population', y='profit', data=data)

x_value = [x for x in range(5,25)]
y_value = [(x * theta[1] + theta [0]) for x in x_value]
sns.lineplot(x_value, y_value)

plt.xlabel("population in 10000s")
plt.ylabel('Profit in $10000s')
plt.Title("Linear Regression Fit")

#inference using optimized theta value
def predict(x, theta):
    y_pred = np.dot(theta.transpose(), x)
    return y_pred

y_pred1 = predict(np.array([1,4]), theta)*10000
print("For a population of 40000 people, the model predicts a profit of $" + str(round(y_pred1,0)))

y_pred2 = predict(np.array([1,8.3]), theta)*10000
print("For a population of 83000 people, the model predicts a profit of $" + str(round(y_pred2,0)))


